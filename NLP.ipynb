{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMM3wOH2BgpFwsQcwiSR2a1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MorrisLesinko/Machine_Learning_Projects/blob/main/NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "62JpJr5oyW2d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# Ensure necessary NLTK resources are downloaded\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Now you can proceed with your imports and the rest of your script\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "\n",
        "# Load the dataset\n",
        "dataset = pd.read_csv(\"/content/SMSSpamCollection\", sep='\\t', names=[\"labels\", \"message\"])\n",
        "\n",
        "# Initialize the lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Preparing the corpus\n",
        "stop_words = set(stopwords.words('english'))\n",
        "corpus = []\n",
        "for message in dataset['message']:\n",
        "    review = re.sub('[^a-zA-Z]', ' ', message).lower().split()\n",
        "    review = [lemmatizer.lemmatize(word) for word in review if word not in stop_words]\n",
        "    corpus.append(' '.join(review))\n",
        "\n",
        "# Vectorization: Creating the TF-IDF model\n",
        "vectorizer = TfidfVectorizer(max_features=2500)\n",
        "X = vectorizer.fit_transform(corpus).toarray()\n",
        "y = dataset['labels']\n",
        "\n",
        "# Label encoding\n",
        "y = pd.get_dummies(y, drop_first=True).values.ravel()\n",
        "\n",
        "# Splitting the datasets into the Training set and Test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)\n",
        "\n",
        "# Training the Multinomial Naive Bayes model on the Training set\n",
        "classifier = MultinomialNB()\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predicting the Test set results\n",
        "y_pred = classifier.predict(X_test)\n",
        "\n",
        "# Making the Confusion Matrix and calculating accuracy\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(cm)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "Hfy-3IYcJjq_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02225803-487d-4cda-bbb0-5d5c253b39cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[954   1]\n",
            " [ 23 137]]\n",
            "Accuracy: 0.97847533632287\n"
          ]
        }
      ]
    }
  ]
}